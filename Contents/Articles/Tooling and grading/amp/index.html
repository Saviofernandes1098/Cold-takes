<!DOCTYPE html>
<html ⚡ lang="en">

<!-- Mirrored from www.cold-takes.com/what-ai-companies-can-do-today-to-help-with-the-most-important-century/amp/ by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 08 Aug 2025 22:04:51 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1">

    <title>What AI companies can do today to help with the most important century</title>

    <meta name="description" content="Major AI companies can increase or reduce global catastrophic risks.">
    <link rel="canonical" href="../index.html">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <meta property="og:site_name" content="Cold Takes">
    <meta property="og:type" content="article">
    <meta property="og:title" content="What AI companies can do today to help with the most important century">
    <meta property="og:description" content="Major AI companies can increase or reduce global catastrophic risks.">
    <meta property="og:url" content="../index.html">
    <meta property="article:published_time" content="2023-02-20T16:58:21.000Z">
    <meta property="article:modified_time" content="2023-09-18T21:48:20.000Z">
    <meta property="article:tag" content="ImplicationsOfMostImportantCentury">
    
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="What AI companies can do today to help with the most important century">
    <meta name="twitter:description" content="Major AI companies can increase or reduce global catastrophic risks.">
    <meta name="twitter:url" content="../index.html">
    <meta name="twitter:label1" content="Written by">
    <meta name="twitter:data1" content="Holden Karnofsky">
    <meta name="twitter:label2" content="Filed under">
    <meta name="twitter:data2" content="ImplicationsOfMostImportantCentury">
    <meta name="twitter:site" content="@coldtakes12">
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "Cold Takes",
        "url": "https://www.cold-takes.com/",
        "logo": {
            "@type": "ImageObject",
            "url": "https://www.cold-takes.com/favicon.ico",
            "width": 48,
            "height": 48
        }
    },
    "author": {
        "@type": "Person",
        "name": "Holden Karnofsky",
        "image": {
            "@type": "ImageObject",
            "url": "https://www.cold-takes.com/content/images/size/w1200/2021/07/HK-headshot-current---compressed-1.jpg",
            "width": 1200,
            "height": 800
        },
        "url": "https://www.cold-takes.com/author/holden/",
        "sameAs": [
            "https://www.cold-takes.com/about/"
        ]
    },
    "headline": "What AI companies can do today to help with the most important century",
    "url": "https://www.cold-takes.com/what-ai-companies-can-do-today-to-help-with-the-most-important-century/",
    "datePublished": "2023-02-20T16:58:21.000Z",
    "dateModified": "2023-09-18T21:48:20.000Z",
    "keywords": "ImplicationsOfMostImportantCentury",
    "description": "\n\n\n\n\n\n\n\n\nI’ve been writing about tangible things we can do today to help the most important century go well. Previously, I wrote about helpful messages to spread and how to help via full-time work.\n\n\n\n\nThis piece is about what major AI companies can do (and not do) to be helpful. By “major AI companies,” I mean the sorts of AI companies that are advancing the state of the art, and/or could play a major role in how very powerful AI systems end up getting used.\n\n\n\n\nThis piece could be useful to pe",
    "mainEntityOfPage": "https://www.cold-takes.com/what-ai-companies-can-do-today-to-help-with-the-most-important-century/"
}
    </script>

    <meta name="generator" content="Ghost 5.130">
    <link rel="alternate" type="application/rss+xml" title="Cold Takes" href="../../rss/index.html">

    <style amp-custom>
    *,
    *::before,
    *::after {
        box-sizing: border-box;
    }

    html {
        overflow-x: hidden;
        overflow-y: scroll;
        font-size: 62.5%;
        -webkit-tap-highlight-color: rgba(0, 0, 0, 0);
    }

    body {
        min-height: 100vh;
        margin: 0;
        padding: 0;
        color: #3a4145;
        font-family: -apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen,Ubuntu,Cantarell,Open Sans,Helvetica Neue,sans-serif;
        font-size: 1.7rem;
        line-height: 1.55em;
        font-weight: 400;
        font-style: normal;
        background: #fff;
        scroll-behavior: smooth;
        overflow-x: hidden;
        -webkit-font-smoothing: antialiased;
        -moz-osx-font-smoothing: grayscale;
    }

    p,
    ul,
    ol,
    li,
    dl,
    dd,
    hr,
    pre,
    form,
    table,
    video,
    figure,
    figcaption,
    blockquote {
        margin: 0;
        padding: 0;
    }

    ul[class],
    ol[class] {
        padding: 0;
        list-style: none;
    }

    img {
        display: block;
        max-width: 100%;
    }

    input,
    button,
    select,
    textarea {
        font: inherit;
        -webkit-appearance: none;
    }

    fieldset {
        margin: 0;
        padding: 0;
        border: 0;
    }

    label {
        display: block;
        font-size: 0.9em;
        font-weight: 700;
    }

    hr {
        position: relative;
        display: block;
        width: 100%;
        height: 1px;
        border: 0;
        border-top: 1px solid currentcolor;
        opacity: 0.1;
    }

    ::selection {
        text-shadow: none;
        background: #cbeafb;
    }

    mark {
        background-color: #fdffb6;
    }

    small {
        font-size: 80%;
    }

    sub,
    sup {
        position: relative;
        font-size: 75%;
        line-height: 0;
        vertical-align: baseline;
    }
    sup {
        top: -0.5em;
    }
    sub {
        bottom: -0.25em;
    }

    ul li + li {
        margin-top: 0.6em;
    }

    a {
        color: var(--ghost-accent-color, #1292EE);
        text-decoration-skip-ink: auto;
    }

    h1,
    h2,
    h3,
    h4,
    h5,
    h6 {
        margin: 0;
        font-weight: 700;
        color: #121212;
        line-height: 1.4em;
    }

    h1 {
        font-size: 3.4rem;
        line-height: 1.1em;
    }

    h2 {
        font-size: 2.4rem;
        line-height: 1.2em;
    }

    h3 {
        font-size: 1.8rem;
    }

    h4 {
        font-size: 1.7rem;
    }

    h5 {
        font-size: 1.6rem;
    }

    h6 {
        font-size: 1.6rem;
    }

    amp-img {
        height: 100%;
        width: 100%;
        max-width: 100%;
        max-height: 100%;
    }

    amp-img img {
        object-fit: cover;
    }
    
    amp-youtube {
        height: calc(100vw / 1.78);
        width: 100vw;
        position: relative;
    }

    amp-youtube img {
        position: absolute;
    }

    .page-header {
        padding: 50px 5vmin 30px;
        text-align: center;
        font-size: 2rem;
        text-transform: uppercase;
        letter-spacing: 0.5px;
    }

    .page-header a {
        color: #121212;
        font-weight: 700;
        text-decoration: none;
        font-size: 1.6rem;
        letter-spacing: -0.1px;
    }

    .post {
        max-width: 680px;
        margin: 0 auto;
    }

    .post-header {
        margin: 0 5vmin 5vmin;
        text-align: center;
    }

    .post-meta {
        margin: 1rem 0 0 0;
        text-transform: uppercase;
        color: #738a94;
        font-weight: 500;
        font-size: 1.3rem;
    }

    .post-image {
        margin: 0 0 5vmin;
    }

    .post-image img {
        display: block;
        width: 100%;
        height: auto;
    }

    .post-content {
        padding: 0 5vmin;
    }

    .post-content > * + * {
        margin-top: 1.5em;
    }

    .post-content [id]:not(:first-child) {
        margin: 2em 0 0;
    }

    .post-content > [id] + * {
        margin-top: 1rem;
    }

    .post-content [id] + .kg-card,
    .post-content blockquote + .kg-card {
        margin-top: 40px;
    }

    .post-content > ul,
    .post-content > ol,
    .post-content > dl {
        padding-left: 1.9em;
    }

    .post-content hr {
        margin-top: 40px;
    }

    .post .post-content hr + * {
        margin-top: 40px;
    }

    .post-content amp-img {
        background-color: #f8f8f8;
    }

    .post-content blockquote {
        position: relative;
        font-style: italic;
    }

    .post-content blockquote::before {
        content: "";
        position: absolute;
        left: -1.5em;
        top: 0;
        bottom: 0;
        width: 0.3rem;
        background: var(--ghost-accent-color, #1292EE);
    }

    .post-content blockquote.kg-blockquote-alt {
        font-size: 1.2em;
        font-style: italic;
        line-height: 1.6em;
        text-align: center;
        color: #738a94;
        padding: 0.75em 3em 1.25em;
    }

    .post-content blockquote.kg-blockquote-alt::before {
        display: none;
    }

    .post-content :not(.kg-card):not([id]) + .kg-card {
        margin-top: 40px;
    }

    .post-content .kg-card + :not(.kg-card) {
        margin-top: 40px;
    }

    .kg-card figcaption {
        padding: 1.5rem 1.5rem 0;
        text-align: center;
        font-weight: 500;
        font-size: 1.3rem;
        line-height: 1.4em;
        opacity: 0.6;
    }

    .kg-card figcaption strong {
        color: rgba(0,0,0,0.8);
    }

    .post-content :not(pre) code {
        vertical-align: middle;
        padding: 0.15em 0.4em 0.15em;
        border: #e1eaef 1px solid;
        font-weight: 400;
        font-size: 0.9em;
        line-height: 1em;
        color: #15171a;
        background: #f0f6f9;
        border-radius: 0.25em;
    }

    .post-content > pre {
        overflow: scroll;
        padding: 16px 20px;
        color: #fff;
        background: #1F2428;
        border-radius: 5px;
        box-shadow: 0 2px 6px -2px rgba(0,0,0,.1), 0 0 1px rgba(0,0,0,.4);
    }

    .kg-embed-card {
        display: flex;
        flex-direction: column;
        align-items: center;
        width: 100%;
    }

    .kg-image-card img {
        margin: auto;
    }

    .kg-gallery-card + .kg-gallery-card {
        margin-top: 0.75em;
    }

    .kg-gallery-container {
        position: relative;
    }

    .kg-gallery-row {
        display: flex;
        flex-direction: row;
        justify-content: center;
    }

    .kg-gallery-image {
        width: 100%;
        height: 100%;
    }

    .kg-gallery-row:not(:first-of-type) {
        margin: 0.75em 0 0 0;
    }

    .kg-gallery-image:not(:first-of-type) {
        margin: 0 0 0 0.75em;
    }

    .kg-bookmark-card,
    .kg-bookmark-publisher {
        position: relative;
    }

    .kg-bookmark-container,
    .kg-bookmark-container:hover {
        display: flex;
        flex-wrap: wrap;
        flex-direction: row-reverse;
        color: currentColor;
        background: rgba(255,255,255,0.6);
        font-family: -apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen,Ubuntu,Cantarell,Open Sans,Helvetica Neue,sans-serif;
        text-decoration: none;
        border-radius: 3px;
        box-shadow: 0 2px 6px -2px rgba(0, 0, 0, 0.1), 0 0 1px rgba(0, 0, 0, 0.4);
        overflow: hidden;
    }

    .kg-bookmark-content {
        flex-basis: 0;
        flex-grow: 999;
        padding: 20px;
        order: 1;
    }

    .kg-bookmark-title {
        font-weight: 600;
        font-size: 1.5rem;
        line-height: 1.3em;
    }

    .kg-bookmark-description {
        display: -webkit-box;
        max-height: 45px;
        margin: 0.5em 0 0 0;
        font-size: 1.4rem;
        line-height: 1.55em;
        overflow: hidden;
        opacity: 0.8;
        -webkit-line-clamp: 2;
        -webkit-box-orient: vertical;
    }

    .kg-bookmark-metadata {
        margin-top: 20px;
    }

    .kg-bookmark-metadata {
        display: flex;
        align-items: center;
        font-weight: 500;
        font-size: 1.3rem;
        line-height: 1.3em;
        white-space: nowrap;
        overflow: hidden;
        text-overflow: ellipsis;
    }

    .kg-bookmark-description {
        display: -webkit-box;
        -webkit-box-orient: vertical;
        -webkit-line-clamp: 2;
        overflow: hidden;
    }

    .kg-bookmark-metadata amp-img {
        width: 18px;
        height: 18px;
        max-width: 18px;
        max-height: 18px;
        margin-right: 10px;
    }

    .kg-bookmark-thumbnail {
        display: flex;
        flex-basis: 20rem;
        flex-grow: 1;
        justify-content: flex-end;
    }

    .kg-bookmark-thumbnail amp-img {
        max-height: 200px;
    }

    .kg-bookmark-author {
        white-space: nowrap;
        text-overflow: ellipsis;
        overflow: hidden;
    }

    .kg-bookmark-publisher::before {
        content: "•";
        margin: 0 .5em;
    }

    .kg-toggle-card-icon {
        display: none;
    }

    .kg-toggle-content {
        margin-top: 0.8rem;
    }

    .kg-product-card-container {
        background: transparent;
        padding: 20px;
        width: 100%;
        border-radius: 5px;
        box-shadow: inset 0 0 0 1px rgb(124 139 154 / 25%);
    }

    .kg-product-card-description p {
        margin-top: 1.5em;
    }

    .kg-product-card-description ul {
        margin-left: 24px;
    }

    .kg-product-card-title {
        font-size: 1.9rem;
        font-weight: 700;
    }

    .kg-product-card-rating-star {
        height: 28px;
        width: 20px;
        margin-right: 2px;
    }

    .kg-product-card-rating-star svg {
    width: 16px;
    height: 16px;
    fill: currentColor;
    opacity: 0.15;
    }

    .kg-product-card-rating-active.kg-product-card-rating-star svg {
    opacity: 1;
    }

    .kg-nft-card-container {
        position: relative;
        display: flex;
        flex: auto;
        flex-direction: column;
        text-decoration: none;
        font-family: -apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen,Ubuntu,Cantarell,Open Sans,Helvetica Neue,sans-serif;
        font-size: 1.4rem;
        font-weight: 400;
        box-shadow: 0 2px 6px -2px rgb(0 0 0 / 10%), 0 0 1px rgb(0 0 0 / 40%);
        width: 100%;
        max-width: 512px;
        color: #15212A;
        background: #fff;
        border-radius: 5px;
        transition: none;
        margin: 0 auto;
    }

    .kg-nft-metadata {
        padding: 2.0rem;
    }

    .kg-nft-image-container {
        position: relative;
    }

    .kg-nft-image {
        display: flex;
        border-radius: 5px 5px 0 0;
    }

    .kg-nft-header {
        display: flex;
        justify-content: space-between;
        align-items: flex-start;
        gap: 20px;
    }

    .kg-nft-header h4.kg-nft-title {
        font-size: 1.9rem;
        font-weight: 700;
        margin: 0;
        color: #15212A;
    }

    .kg-nft-header amp-img {
        max-width: 114px;
        max-height: 26px;
    }

    .kg-nft-opensea-logo {
        margin-top: 2px;
        width: 100px;
    }

    .kg-nft-creator {
        font-family: inherit;
        color: #95A1AD;
    }

    .kg-nft-creator span {
        font-weight: 500;
        color: #15212A;
    }

    .kg-nft-card p.kg-nft-description {
        font-size: 1.4rem;
        line-height: 1.4em;
        margin: 2.0rem 0 0;
        color: #222;
    }

    .kg-button-card {
        display: flex;
        position: static;
        align-items: center;
        width: 100%;
        justify-content: center;
    }

    .kg-btn {
        display: flex;
        position: static;
        align-items: center;
        padding: 0 2.0rem;
        height: 4.0rem;
        line-height: 4.0rem;
        font-size: 1.65rem;
        font-weight: 600;
        text-decoration: none;
        border-radius: 5px;
        transition: opacity 0.2s ease-in-out;
    }

    .kg-btn:hover {
        opacity: 0.85;
    }

    .kg-btn-accent {
        background-color: var(--ghost-accent-color, #1292EE);
        color: #fff;
    }

    .kg-callout-card {
        display: flex;
        padding: 20px 28px;
        border-radius: 3px;
    }

    .kg-callout-card-grey {
        background: rgba(124, 139, 154, 0.13);
    }

    .kg-callout-card-white {
        background: transparent;
        box-shadow: inset 0 0 0 1px rgba(124, 139, 154, 0.25);
    }

    .kg-callout-card-blue {
        background: rgba(33, 172, 232, 0.12);
    }

    .kg-callout-card-green {
        background: rgba(52, 183, 67, 0.12);
    }

    .kg-callout-card-yellow {
        background: rgba(240, 165, 15, 0.13);
    }

    .kg-callout-card-red {
        background: rgba(209, 46, 46, 0.11);
    }

    .kg-callout-card-pink {
        background: rgba(225, 71, 174, 0.11);
    }

    .kg-callout-card-purple {
        background: rgba(135, 85, 236, 0.12);
    }

    .kg-callout-card-accent {
        background: var(--ghost-accent-color);
        color: #fff;
    }

    .kg-callout-card-accent a {
        color: #fff;
    }

    .kg-callout-emoji {
        padding-right: 16px;
        line-height: 1.3;
        font-size: 1.25em;
    }

    .kg-header-card {
        padding: 6em 3em;
        display: flex;
        flex-direction: column;
        align-items: center;
        justify-content: center;
        text-align: center;
    }

    .kg-header-card.kg-size-small {
        padding-top: 4em;
        padding-bottom: 4em;
    }

    .kg-header-card.kg-size-large {
        padding-top: 12em;
        padding-bottom: 12em;
    }

    .kg-header-card.kg-width-full {
        padding-left: 4em;
        padding-right: 4em;
    }

    .kg-header-card.kg-align-left {
        text-align: left;
        align-items: flex-start;
    }

    .kg-header-card.kg-style-dark {
        background: #15171a;
        color: #ffffff;
    }

    .kg-header-card.kg-style-light {
        color: #15171a;
        border: 1px solid rgba(124, 139, 154, 0.25);
        border-width: 1px 0;
    }

    .kg-header-card.kg-style-accent {
        background-color: var(--ghost-accent-color);
    }

    .kg-header-card.kg-style-image {
        background-color: #e7e7eb;
        background-size: cover;
        background-position: center center;
    }

    .kg-header-card h2 {
        font-size: 4em;
        font-weight: 700;
        line-height: 1.1em;
        margin: 0;
    }

    .kg-header-card h2 strong {
        font-weight: 800;
    }

    .kg-header-card.kg-size-small h2 {
        font-size: 3em;
    }

    .kg-header-card.kg-size-large h2 {
        font-size: 5em;
    }

    .kg-header-card h3 {
        font-size: 1.25em;
        font-weight: 500;
        line-height: 1.3em;
        margin: 0;
    }

    .kg-header-card h3 strong {
        font-weight: 600;
    }

    .kg-header-card.kg-size-small h3 {
        font-size: 1em;
    }

    .kg-header-card.kg-size-large h3 {
        font-size: 1.5em;
    }

    .kg-header-card:not(.kg-style-light) h2,
    .kg-header-card:not(.kg-style-light) h3 {
        color: #ffffff;
    }

    .kg-header-card a.kg-header-card-button {
        display: flex;
        position: static;
        align-items: center;
        padding: 0 1.2em;
        height: 2.4em;
        line-height: 1em;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Roboto", "Oxygen", "Ubuntu", "Cantarell", "Fira Sans", "Droid Sans", "Helvetica Neue", sans-serif;
        font-size: 0.95em;
        font-weight: 600;
        text-decoration: none;
        border-radius: 5px;
        transition: opacity 0.2s ease-in-out;
        background-color: var(--ghost-accent-color);
        color: #ffffff;
        margin: 1.75em 0 0;
    }

    .kg-header-card a.kg-header-card-button:hover {
        opacity: 0.85;
    }

    .kg-header-card.kg-size-large a.kg-header-card-button {
        margin-top: 2em;
    }

    .kg-header-card.kg-size-small a.kg-header-card-button {
        margin-top: 1.5em;
    }

    .kg-header-card.kg-style-image a.kg-header-card-button,
    .kg-header-card.kg-style-dark a.kg-header-card-button {
        background: #ffffff;
        color: #15171a;
    }

    .kg-header-card.kg-style-accent a.kg-header-card-button {
        background: #ffffff;
        color: var(--ghost-accent-color);
    }

    .kg-audio-card {
        display: flex;
        width: 100%;
        box-shadow: inset 0 0 0 1px rgba(124, 139, 154, 0.25);
    }

    .kg-audio-thumbnail {
        display: flex;
        justify-content: center;
        align-items: center;
        width: 80px;
        min-width: 80px;
        height: 80px;
        background: transparent;
        object-fit: cover;
        aspect-ratio: 1/1;
        border-radius: 3px 0 0 3px;
    }

    .kg-audio-thumbnail.placeholder {
        background: var(--ghost-accent-color);
    }

    .kg-audio-thumbnail.placeholder svg {
        width: 24px;
        height: 24px;
        fill: white;
    }

    .kg-audio-player-container {
        position: relative;
        display: flex;
        flex-direction: column;
        justify-content: space-between;
        width: 100%;
        --seek-before-width: 0%;
        --volume-before-width: 100%;
        --buffered-width: 0%;
    }

    .kg-audio-title {
        width: 100%;
        padding: 8px 12px 0;
        border: none;
        font-family: inherit;
        font-size: 1.1em;
        font-weight: 700;
        background: transparent;
    }

    .kg-audio-player {
        display: none;
    }

    .kg-width-full.kg-card-hascaption {
        display: grid;
        grid-template-columns: inherit;
    }

    .post-content table {
        border-collapse: collapse;
        width: 100%;
    }

    .post-content th {
        padding: 0.5em 0.8em;
        text-align: left;
        font-size: .75em;
        text-transform: uppercase;
    }

    .post-content td {
        padding: 0.4em 0.7em;
    }

    .post-content tbody tr:nth-child(2n + 1) {
        background-color: rgba(0,0,0,0.1);
        padding: 1px;
    }

    .post-content tbody tr:nth-child(2n + 2) td:last-child {
        box-shadow:
            inset 1px 0 rgba(0,0,0,0.1),
            inset -1px 0 rgba(0,0,0,0.1);
    }

    .post-content tbody tr:nth-child(2n + 2) td {
        box-shadow: inset 1px 0 rgba(0,0,0,0.1);
    }

    .post-content tbody tr:last-child {
        border-bottom: 1px solid rgba(0,0,0,.1);
    }

    .page-footer {
        padding: 60px 5vmin;
        margin: 60px auto 0;
        text-align: center;
        background-color: #f8f8f8;
    }

    .page-footer h3 {
        margin: 0.5rem 0 0 0;
    }

    .page-footer p {
        max-width: 500px;
        margin: 1rem auto 1.5rem;
        font-size: 1.7rem;
        line-height: 1.5em;
        color: rgba(0,0,0,0.6)
    }

    .powered {
        display: inline-flex;
        align-items: center;
        margin: 30px 0 0;
        padding: 6px 9px 6px 6px;
        border: rgba(0,0,0,0.1) 1px solid;
        font-size: 12px;
        line-height: 12px;
        letter-spacing: -0.2px;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Roboto", "Oxygen", "Ubuntu", "Cantarell", "Fira Sans", "Droid Sans", "Helvetica Neue", sans-serif;
        font-weight: 500;
        color: #222;
        text-decoration: none;
        background: #fff;
        border-radius: 6px;
    }

    .powered svg {
        height: 16px;
        width: 16px;
        margin: 0 6px 0 0;
    }

    @media (max-width: 600px) {
        body {
            font-size: 1.6rem;
        }
        h1 {
            font-size: 3rem;
        }

        h2 {
            font-size: 2.2rem;
        }
    }

    @media (max-width: 400px) {
        h1 {
            font-size: 2.6rem;
            line-height: 1.15em;
        }
        h2 {
            font-size: 2rem;
            line-height: 1.2em;
        }
        h3 {
            font-size: 1.7rem;
        }
    }

    :root {--ghost-accent-color: #888888;}
    </style>

    <style amp-boilerplate>body{-webkit-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-moz-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-ms-animation:-amp-start 8s steps(1,end) 0s 1 normal both;animation:-amp-start 8s steps(1,end) 0s 1 normal both}@-webkit-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-moz-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-ms-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-o-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}</style><noscript><style amp-boilerplate>body{-webkit-animation:none;-moz-animation:none;-ms-animation:none;animation:none}</style></noscript>
    <script async src="../../../cdn.ampproject.org/v0.js"></script>

    

</head>

<body class="amp-template">
    <header class="page-header">
        <a href="../../index.html">
                Cold Takes
        </a>
    </header>

    <main class="content" role="main">
        <article class="post">

            <header class="post-header">
                <h1 class="post-title">What AI companies can do today to help with the most important century</h1>
                <section class="post-meta">
                    Holden Karnofsky -
                    <time class="post-date" datetime="2023-02-20">20 Feb 2023</time>
                </section>
            </header>
            <section class="post-content">

                <p></p><div id="buzzsprout-player-12274101"></div>
<figcaption><em>Click lower right to download or find on Apple Podcasts, Spotify, Stitcher, etc.</em></figcaption><p></p>


<p>
I’ve been writing about tangible things we can do today to help the <a href="../../most-important-century/index.html">most important century</a> go well. Previously, I wrote about <a href="../../spreading-messages-to-help-with-the-most-important-century/index.html">helpful messages to spread</a> and <a href="../../jobs-that-can-help-with-the-most-important-century/index.html">how to help via full-time work</a>.
</p>
<p>
This piece is about what major AI companies can do (and not do) to be helpful. By “major AI companies,” I mean the sorts of AI companies that are advancing the state of the art, and/or could play a major role in how very powerful AI systems end up getting used.<sup id="fnref1"><a href="../../p/f19236c6-34b8-4487-a458-0fc8fe00fb37/index.html#fn1" rel="footnote">1</a></sup>
</p>
<p>
This piece could be useful to people who work at those companies, or people who are just curious.
</p>
<p>
Generally, these are not pie-in-the-sky suggestions - I can name<sup id="fnref2"><a href="../../p/f19236c6-34b8-4487-a458-0fc8fe00fb37/index.html#fn2" rel="footnote">2</a></sup> more than one AI company that has at least made a serious effort at each of the things I discuss below<strong> </strong>(beyond what it would do if everyone at the company were singularly focused on making a profit).<sup id="fnref3"><a href="../../p/f19236c6-34b8-4487-a458-0fc8fe00fb37/index.html#fn3" rel="footnote">3</a></sup>
</p>
<p>
I’ll cover:
</p>
<ul>

<li>Prioritizing alignment research, strong security, and safety standards (all of which I’ve written about <a href="../../how-we-could-stumble-into-ai-catastrophe/index.html#we-can-do-better">previously</a>).

</li><li>Avoiding hype and acceleration, which I think could leave us with less time to prepare for key risks.

</li><li>Preparing for difficult decisions ahead: setting up governance, employee expectations, investor expectations, etc. so that the company is capable of doing non-profit-maximizing things to help avoid catastrophe in the future.

</li><li>Balancing these cautionary measures with conventional/financial success.

</li><li>I’ll also list a few things that some AI companies present as important, but which I’m less excited about, e.g. censorship of AI models, and raising awareness of AI with governments and the public. I don’t think all these things are necessarily <em>bad</em>, but I think some are, and I’m skeptical that any are crucial for the <a href="../../tag/implicationsofmostimportantcentury/index.html">risks I’ve focused on</a>.
</li>
</ul>
<p>
I previously laid out a summary of how I see the major risks of advanced AI, and four key things I think can help (<span><strong>alignment research</strong></span>;<strong> </strong><span><strong>strong security</strong></span>; <span><strong>standards and monitoring</strong></span>; <span><strong>successful, careful AI projects</strong></span>). I won’t repeat that summary now, but it might be helpful for orienting you if you don’t remember the rest of this series too well; click <a href="../../jobs-that-can-help-with-the-most-important-century/index.html#recap">here</a> to read it.
</p>
<h2 id="basics">Some basics: alignment research, strong security, safety standards</h2>


<p>
First off, AI companies can contribute to the “things that can help” I listed above:
</p>
<ul>

<li>They can prioritize <span><strong>alignment research</strong></span><strong> </strong>(and <a href="../../jobs-that-can-help-with-the-most-important-century/index.html#other-technical-research">other technical research</a>, e.g. threat assessment research and misuse research).  
<ul>
 
<li>For example, they can prioritize hiring for safety teams, empowering these teams, encouraging their best flexible researchers to work on safety, aiming for high-quality research that targets <a href="../../ai-safety-seems-hard-to-measure/index.html">crucial challenges</a>, etc.
 
</li><li>It could also be important for AI companies to find ways to <strong>partner with outside safety researchers rather than rely solely on their own teams.</strong> As discussed <a href="../../jobs-that-can-help-with-the-most-important-century/index.html#SafetyCollaborations">previously</a>, this could be challenging. But I generally expect that AI companies that care a lot about safety research partnerships will find ways to make them work.
</li> 
</ul>
    </li><li>They can help work toward a <span><strong>standards and monitoring</strong></span><strong> </strong>regime. E.g., they can do their own work to come up with standards like "An AI system is dangerous if we observe that it's able to ___, and if we observe this we will take safety and security measures such as ____." They can also consult with others developing safety standards, voluntarily self-regulate beyond what’s required by law, etc.
</li>


<li>They can prioritize <span><strong>strong security</strong></span>, beyond what normal commercial incentives would call for.  
<ul>
 
<li>It could easily take years to build secure enough systems, processes and technologies for very high-stakes AI.
 
</li><li>It could be important to hire not only people to handle everyday security needs, but people to experiment with more exotic setups that could be needed later, as the incentives to steal AI get stronger.
</li> 
</ul>

</li></ul>
(Click to expand) The challenge of securing dangerous AI<div>

<p>In <a href="../../racing-through-a-minefield-the-ai-deployment-problem/index.html">Racing Through a Minefield</a>, I described a "race" between cautious actors (those who take <a href="../../why-would-ai-aim-to-defeat-humanity/index.html">misalignment risk</a> seriously) and incautious actors (those who are focused on deploying AI for their own gain, and aren't thinking much about the dangers to the whole world). Ideally, cautious actors would collectively have more powerful AI systems than incautious actors, so they could take their time doing <a href="../../high-level-hopes-for-ai-alignment/index.html">alignment research</a> and <a href="../../racing-through-a-minefield-the-ai-deployment-problem/index.html#defensive-deployment">other things</a> to try to make the situation safer for everyone. </p>

<p>But if incautious actors can steal an AI from cautious actors and rush forward to deploy it for their own gain, then the situation looks a lot bleaker. And unfortunately, it could be hard to protect against this outcome.</p>

<p>It's generally <a href="../../ai-could-defeat-all-of-us-combined/index.html#fn15">extremely difficult</a> to protect data and code against a well-resourced cyberwarfare/espionage effort. An AI’s “weights” (you can think of this sort of like its source code, though <a href="../../p/97d2a7b1-af2d-4dd4-b679-5ea8bb41c47d/index.html#fn4">not exactly</a>) are potentially very dangerous on their own, and hard to get extreme security for. Achieving enough cybersecurity could require measures, and preparations, well beyond what one would normally aim for in a commercial context.</p></div>


(Click to expand) How standards might be established and become national or international<div>

<p>
I <a href="../../racing-through-a-minefield-the-ai-deployment-problem/index.html#global-monitoring">previously</a> laid out a possible vision on this front, which I’ll give a slightly modified version of here:
</p>
<ul>

<li>Today’s leading AI companies could self-regulate by committing not to build or deploy a system that they can’t convincingly demonstrate is safe (e.g., see Google’s <a href="https://www.theweek.in/news/sci-tech/2018/06/08/google-wont-deploy-ai-to-build-military-weapons-ichai.html">2018 statement</a>, "We will not design or deploy AI in weapons or other technologies whose principal purpose or implementation is to cause or directly facilitate injury to people”).  
<ul>
 
<li>Even if some people at the companies would like to deploy unsafe systems, it could be hard to pull this off once the company has committed not to. 
 
</li><li>Even if there’s a lot of room for judgment in what it means to demonstrate an AI system is safe, having agreed in advance that <a href="../../ai-safety-seems-hard-to-measure/index.html">certain evidence</a> is <em>not</em> good enough could go a long way.
</li> 
</ul>

</li><li>As more AI companies are started, they could feel soft pressure to do similar self-regulation, and refusing to do so is off-putting to potential employees, investors, etc.

</li><li>Eventually, similar principles could be incorporated into various government regulations and enforceable treaties.

</li><li>Governments could monitor for dangerous projects using regulation and even overseas operations. E.g., today the US monitors (without permission) for various signs that other states might be developing nuclear weapons, and might try to stop such development with methods ranging from threats of sanctions to <a href="https://en.wikipedia.org/wiki/Stuxnet">cyberwarfare</a> or even military attacks. It could do something similar for any AI development projects that are using huge amounts of compute and haven’t volunteered information about whether they’re meeting standards.
</li>
    </ul></div>


<h2 id="avoiding-hype">Avoiding hype and acceleration </h2>


<p>
It seems good for AI companies to <strong>avoid</strong> <strong>unnecessary hype and acceleration of AI. </strong>
</p>
<p>
I’ve argued that <a href="../../spreading-messages-to-help-with-the-most-important-century/index.html#were-not-ready-for-this">we’re not ready</a> for transformative AI, and I generally tend to think that we’d all be better off if the world took <em>longer</em> to develop transformative AI. That’s because:
</p>
<p>
 
</p>
<ul>

<li>I’m hoping general awareness and understanding of the key risks will rise over time.

</li><li>A lot of key things that could improve the situation - e.g., <span><strong>alignment research</strong></span>, <span><strong>standards and monitoring</strong></span>, and <span><strong>strong security</strong></span><strong> </strong>- seem to be in very early stages right now.

</li><li>If too much money pours into the AI world too fast, I’m worried there will be lots of <a href="../../racing-through-a-minefield-the-ai-deployment-problem/index.html#basic-premises">incautious</a> companies racing to build transformative AI as quickly as they can, with little regard for the key risks.
</li>
</ul>
<p>
By default, I generally think: “The fewer flashy demos and breakthrough papers a lab is putting out, the better.” This can involve tricky tradeoffs in practice (since AI companies generally want to be successful at recruiting, fundraising, etc.)
</p><p>
    A couple of potential counterarguments, and replies:</p>

<p>First, some people think it's now "too late" to avoid hype and acceleration, given the amount of hype and investment AI is getting at the moment. I disagree. It's easy to forget, in the middle of a media cycle, how quickly people can forget about things and move onto the next story once the bombs stop dropping. And there are plenty of bombs that still haven't dropped (many things AIs still can't do), and the level of investment in AI has tons of room to go up from here.</p>
<p>Second, I’ve sometimes seen arguments that hype is <em>good</em> because it helps society at large understand what’s coming. But unfortunately, as I wrote <a href="../../spreading-messages-to-help-with-the-most-important-century/index.html#challenges-of-ai-related-messages">previously</a>, I'm worried that hype gives people a skewed picture.</p><ul>
    <li>Some <a href="../../why-would-ai-aim-to-defeat-humanity/index.html">key risks</a> are hard to understand and take seriously.
        </li><li>What's easy to understand is something like "AI is powerful and scary, I should make sure that people like me are the ones to build it!"
            </li><li>Maybe <a href="https://twitter.com/sethlazar/status/1626257535178280960">recent developments</a> will make people understand the risks better? One can hope, but I'm not counting on that just yet - I think AI misbehavior can be <a href="../../how-we-could-stumble-into-ai-catastrophe/index.html#how-we-could-stumble-into-catastrophe-from-misaligned-ai">given illusory "fixes,"</a> and probably will be.</li></ul>

<p>I also am generally skeptical that there's much hope of society adapting to risks as they happen, given the <a href="../../most-important-century/index.html">explosive pace of change</a> that I expect once we get powerful enough AI systems.</p>

<p>I discuss some more arguments on this point in a footnote.<sup id="fnref4"><a href="../../p/f19236c6-34b8-4487-a458-0fc8fe00fb37/index.html#fn4" rel="footnote">4</a></sup></p>

    <p>
I don’t think it’s clear-cut that hype and acceleration are bad, but it’s my best guess.
</p>
<h2 id="preparing-for-difficult-decisions">Preparing for difficult decisions ahead</h2>


<p>
I’ve <a href="../../racing-through-a-minefield-the-ai-deployment-problem/index.html">argued</a> that AI companies might need to do “out-of-the-ordinary” things that don’t go with normal commercial incentives. 
</p>
<p>
Today, AI companies can be building a foundation for being able to do “out-of-the-ordinary” things in the future. A few examples of how they might do so:
</p>
<p>
<strong>Public-benefit-oriented governance. </strong>I think typical governance structures could be a problem in the future. For example, a standard corporation could be sued for <em>not</em> deploying AI that poses a risk of <a href="../../ai-could-defeat-all-of-us-combined/index.html">global catastrophe</a> - if this means a sacrifice for its bottom line.
</p>
<p>
I’m excited about AI companies that are investing heavily in setting up governance structures - and investing in executives and board members - capable of making the hard calls well. For example:
</p>
<ul>

<li>By default, if an AI company is a standard corporation, its leadership has legally recognized <a href="https://en.wikipedia.org/wiki/Fiduciary">duties</a> to serve the interests of shareholders - not society at large. But an AI company can incorporate as a <a href="https://www.delawareinc.com/public-benefit-corporation/">Public Benefit Corporation</a> or some other kind of entity (including a nonprofit!) that gives more flexibility here.

</li><li>By default, shareholders make the final call over what a company does. (Shareholders can replace members of the Board of Directors, who in turn can replace the CEO). But a company can set things up differently (e.g., a <a href="https://openai.com/blog/openai-lp/">for-profit controlled by a nonprofit</a><sup id="fnref5"><a href="../../p/f19236c6-34b8-4487-a458-0fc8fe00fb37/index.html#fn5" rel="footnote">5</a></sup>).</li></ul>
<p>
It could pay off in lots of ways to make sure the final calls at a company are made by people focused on getting a good outcome for humanity (and legally free to focus this way).
</p>
<p>
<strong>Gaming out the future. </strong>I think it’s not too early for AI companies to be discussing how they would handle various <a href="../../racing-through-a-minefield-the-ai-deployment-problem/index.html">high-stakes situations</a>.
</p>
<ul>

<li>Under what circumstances would the company simply decide to stop training increasingly powerful AI models? 

</li><li>If the company came to believe it was building very powerful, dangerous models, whom would it notify and seek advice from? At what point would it approach the government, and how would it do so?

</li><li>At what point would it be worth using extremely costly security measures?

</li><li>If the company had AI systems available that could do most of what humans can do, what would it <em>do</em> with these systems? Use them to do AI safety research? Use them to design better algorithms and continue making increasingly powerful AI systems? (More possibilities <a href="../../racing-through-a-minefield-the-ai-deployment-problem/index.html#defensive-deployment">here</a>.)

</li><li>Who should be leading the way on decisions like these? Companies tend to employ experts to inform their decisions; who would the company look to for expertise on these kinds of decisions?
</li>
</ul>
<p>
<strong>Establishing and getting practice with processes for particularly hard decisions. </strong>Should the company publish its latest research breakthrough? Should it put out a product that might lead to more <a href="../../p/f19236c6-34b8-4487-a458-0fc8fe00fb37/index.html#avoiding-hype">hype and acceleration</a>? What safety researchers should <a href="../../jobs-that-can-help-with-the-most-important-century/index.html#SafetyCollaborations">get access to its models</a>, and how much access? 
</p>
<p>
AI companies face questions like this pretty regularly today, and I think it’s worth putting processes in place to consider the implications for the world as a whole (not just for the company’s bottom line). This could include assembling advisory boards, internal task forces, etc.
</p>
<p>
<strong>Managing employee and investor expectations. </strong>At some point, an AI company might want to make “out of the ordinary” moves that are good for the world but bad for the bottom line. E.g., choosing not to deploy AIs that could be very dangerous or very profitable.
</p>
<p>
I wouldn’t want to be trying to run a company in this situation with lots of angry employees and investors asking about the value of their equity shares! It’s also important to minimize the risk of employees and/or investors leaking sensitive and potentially <a href="../../p/f19236c6-34b8-4487-a458-0fc8fe00fb37/index.html#Box1">dangerous</a> information.
</p>
<p>
AI companies can prepare for this kind of situation by doing things like:
</p>
<ul>

<li>Being selective about whom they hire and take investment from, and screening specifically for people they think are likely to be on board with these sorts of hard calls.

</li><li>Education and communications - making it clear to employees what kinds of dangerous-to-humanity situations might be coming up in the future, and what kinds of actions the company might want to take (and why).
</li>
</ul>
<p>
<strong>Internal and external commitments. </strong>AI companies can make public and/or internal statements about how they would handle various tough situations, e.g. how they would determine when it’s too dangerous to keep building more powerful models. 
</p>
<p>
I think these commitments should generally be non-binding (it’s hard to predict the future in enough detail to make binding ones). But in a future where maximizing profit conflicts with doing the right thing for humanity, a previously-made commitment could make it more likely that the company does the right thing.
</p>
<h2 id="succeeding">Succeeding</h2>


<p>
I’ve emphasized how helpful a <span><strong>successful, careful AI projects</strong></span><strong> </strong>could be. So far, this piece has mostly talked about the “careful” side of things - how to do things that a “normal” AI company (focused only on commercial success) wouldn’t, in order to reduce risks. But it’s also important to succeed at fundraising, recruiting, and generally staying relevant (e.g., capable of building cutting-edge AI systems). 
</p>
<p>
I don’t emphasize this or write about it as much because I think it’s the sort of thing AI companies are likely to be focused on by default, and because I don’t have special insight into how to succeed as an AI company. But it’s important, and it means that AI companies need to walk a sort of tightrope - constantly making tradeoffs between success and caution.
</p>
<h2 id="some-things-im-less-excited-about">Some things I’m less excited about</h2>


<p>
I think it’s also worth listing a few things that some AI companies present as important societal-benefit measures, but which I’m a bit more skeptical are crucial for reducing the risks I’ve <a href="../../tag/implicationsofmostimportantcentury/index.html">focused on</a>.
</p>
<ul>

<li>Some AI companies restrict access to their models so people won’t use the AIs to create pornography, misleading images and text, etc. I’m not necessarily against this and support versions of it (it depends on the details), but I mostly don’t think it is a key way to reduce the risks I’ve focused on. For those risks, the hype that comes from seeing a demonstration of a system’s capabilities could be even <a href="../../p/f19236c6-34b8-4487-a458-0fc8fe00fb37/index.html#avoiding-hype">more dangerous</a> than direct harms.

</li><li>I sometimes see people implying that open-sourcing AI models - and otherwise making them as broadly available as possible - is a key social-benefit measure. While there may be benefits in some cases, I don't tend to think this is highly beneficial for the <a href="../../tag/implicationsofmostimportantcentury/index.html">risks I’m most concerned about</a> (though it could be beneficial in other ways).  
<ul>
 
<li>I think it can contribute to <a href="../../p/f19236c6-34b8-4487-a458-0fc8fe00fb37/index.html#avoiding-hype">hype and acceleration</a>, and could make it generally harder to enforce safety standards. 
 
</li><li>In the long run, I worry that AI systems could become extraordinarily powerful (more so than e.g. nuclear weapons), so I don’t think “Make sure everyone has access asap” is the right framework for the long run. 
 
</li><li>In addition to increasing dangers from misaligned AI, this framework could increase other dangers I’ve <a href="../../how-we-could-stumble-into-ai-catastrophe/index.html#potential-catastrophes-from-aligned-ai">written about previously</a>.
</li> 
</ul>

</li><li>I generally don’t think AI companies should be trying to get governments to pay more attention to AI, for reasons I’ll get to in a future piece. (Forming relationships with policymakers could be good, though.)

</li></ul>
<p>
When an AI company presents some decision as being for the benefit of humanity, I often ask myself, “Could this same decision be justified by just wanting to commercialize successfully?”
</p>
<p>
For example, making AI models “safe” in the sense that they <em>usually behave as users intend </em>(including things like refraining from toxic language, chaotic behavior, etc.) can be important for commercial viability, but <a href="../../why-would-ai-aim-to-defeat-humanity/index.html#why-we-might-not-get-clear-warning-signs">isn’t necessarily good enough for the risks I worry about</a>.
</p>


<p></p><div>

        <span><a href="https://api.addthis.com/oexchange/0.8/forward/twitter/offer?url=https%3A%2F%2Fwww.cold-takes.com%2Fwhat-ai-companies-can-do-today-to-help-with-the-most-important-century&amp;pubid=ra-60a178324cffc42e&amp;title=Cold%20Takes%20-%20What%20AI%20companies%20can%20do%20today%20to%20help%20with%20the%20most%20important%20century&amp;ct=1" target="_blank"><amp-img width="64" src="../../content/images/2021/06/ct-twitter-square.png" alt="Twitter" height="64" layout="fixed"></amp-img></a></span><span>
        <a href="https://api.addthis.com/oexchange/0.8/forward/facebook/offer?url=https%3A%2F%2Fwww.cold-takes.com%2Fwhat-ai-companies-can-do-today-to-help-with-the-most-important-century&amp;pubid=ra-60a178324cffc42e&amp;title=Cold%20Takes%20-%20What%20AI%20companies%20can%20do%20today%20to%20help%20with%20the%20most%20important%20century&amp;ct=1" target="_blank"><amp-img width="64" src="../../content/images/2021/06/ct-facebook-square.png" alt="Facebook" height="64" layout="fixed"></amp-img></a></span><span>
        <a href="https://api.addthis.com/oexchange/0.8/forward/reddit/offer?url=https%3A%2F%2Fwww.cold-takes.com%2Fwhat-ai-companies-can-do-today-to-help-with-the-most-important-century&amp;pubid=ra-60a178324cffc42e&amp;title=Cold%20Takes%20-%20What%20AI%20companies%20can%20do%20today%20to%20help%20with%20the%20most%20important%20century&amp;ct=1" target="_blank"><amp-img width="64" src="../../content/images/2021/06/ct-reddit-square.png" alt="Reddit" height="64" layout="fixed"></amp-img></a></span><span>
        <a href="https://api.addthis.com/oexchange/0.8/forward/menu/offer?url=https%3A%2F%2Fwww.cold-takes.com%2Fwhat-ai-companies-can-do-today-to-help-with-the-most-important-century&amp;pubid=ra-60a178324cffc42e&amp;title=Cold%20Takes%20-%20What%20AI%20companies%20can%20do%20today%20to%20help%20with%20the%20most%20important%20century&amp;ct=1" target="_blank"><amp-img width="64" src="../../content/images/2021/06/ct-addthis-square.png" alt="More" height="64" layout="fixed"></amp-img></a></span>
        </div>
<center><p id="discuss"> <a href="https://www.lesswrong.com/posts/slug/what-ai-companies-can-do-today-to-help-with-the-most-important-century#comments" target="_blank"><button class="button">Comment/discuss</button></a></p><p></p></center>

<h2 id="footnotes">Footnotes</h2>
<div class="footnotes">
<hr></hr>
<ol><li id="fn1">
<p>
     Disclosure: my wife works at one such company (<a href="https://anthropic.com/">Anthropic</a>) and used to work at another (<a href="https://openai.com/">OpenAI</a>), and has equity in both. <a href="#fnref1">↩</a><li id="fn2">
<p>
     Though I won’t, because I decided I don’t want to get into a thing about whom I did and didn’t link to. Feel free to give real-world examples in the comments! <a href="#fnref2">↩</a><li id="fn3">
<p>
     Now, AI companies could sometimes be doing “responsible” or “safety-oriented” things in order to get good PRs, recruit employees, make existing employees happy, etc. In this sense, the actions could be <em>ultimately</em> profit-motivated. But that would still mean there are <em>enough people who care about reducing AI risk that actions like these have PR benefits, recruiting benefits, etc. </em>That’s a big deal! And it suggests that if concern about AI risks (and understanding of how to reduce them) were more widespread, AI companies might do more good things and fewer dangerous things. <a href="#fnref3">↩</a><li id="fn4">
<p>
     You could argue that it would be better for the world to develop extremely powerful AI systems <em>sooner</em>, for reasons including:
</p><ul>

<li>You might be pretty happy with the global balance of power between countries today, and be worried that it’ll get worse in the future. The latter could lead to a situation where the “wrong” government <a href="../../transformative-ai-issues-not-just-misalignment-an-overview/index.html#power-imbalances">leads the way on transformative AI</a>.

</li><li>You might think that the later we develop transformative AI, the more quickly everything will play out, because there will be more computing resources available in the world. E.g., if we develop extremely powerful systems tomorrow, there would only be so many copies we could run at once, whereas if we develop equally powerful systems in 50 years, it might be a lot easier for lots of people to run lots of copies. (More: <a href="https://aiimpacts.org/hardware-overhang/">Hardware Overhang</a>)</li></ul>

<p>
    A key reason I believe it’s best to avoid acceleration at this time is because it seems plausible (at least 10% likely) that transformative AI will be developed <em>extremely</em> soon - as in within 10 years of today. My  impression is that many people at major AI companies tend to agree with this. I think this is a very scary possibility, and if this is the case, the arguments I give in the main text seem particularly important (e.g., many key interventions seem to be in a pretty embryonic state, and awareness of key risks seems low).
</p><p>
    A related case one could make for acceleration is “It’s worth accelerating things on the whole to increase the probability that the particular company in question succeeds” (more here: the <a href="../../making-the-best-of-the-most-important-century/index.html#the-competition-frame">“competition” frame</a>). I think this is a valid consideration, which is why I talk about tricky tradeoffs in the main text. <a href="#fnref4">↩</a><li id="fn5">

<p>
     Note that my wife is a former employee of OpenAI, the company I link to there, and she owns equity in the company. <a href="#fnref5">↩</a>
</p></li></p></li></p></li></p></li></p></li></ol></div>



            </section>

        </article>
    </main>
    <footer class="page-footer">
        <h3>Cold Takes</h3>
            <p>For audio version, search for &quot;Cold Takes Audio&quot; in your podcast app</p>
        <p><a href="../../index.html">Read more posts →</a></p>
        <a class="powered" href="https://ghost.org/" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 156 156"><g fill="none" fill-rule="evenodd"><rect fill="#15212B" width="156" height="156" rx="27"/><g transform="translate(36 36)" fill="#F6F8FA"><path d="M0 71.007A4.004 4.004 0 014 67h26a4 4 0 014 4.007v8.986A4.004 4.004 0 0130 84H4a4 4 0 01-4-4.007v-8.986zM50 71.007A4.004 4.004 0 0154 67h26a4 4 0 014 4.007v8.986A4.004 4.004 0 0180 84H54a4 4 0 01-4-4.007v-8.986z"/><rect y="34" width="84" height="17" rx="4"/><path d="M0 4.007A4.007 4.007 0 014.007 0h41.986A4.003 4.003 0 0150 4.007v8.986A4.007 4.007 0 0145.993 17H4.007A4.003 4.003 0 010 12.993V4.007z"/><rect x="67" width="17" height="17" rx="4"/></g></g></svg> Published with Ghost</a>
    </footer>
    
</body>

<!-- Mirrored from www.cold-takes.com/what-ai-companies-can-do-today-to-help-with-the-most-important-century/amp/ by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 08 Aug 2025 22:04:51 GMT -->
</html>
